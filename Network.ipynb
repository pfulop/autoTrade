{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import pandas\n",
    "from itertools import count\n",
    "import math\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.DataFrame.from_csv('final.csv')\n",
    "data = data.values\n",
    "prices = pandas.DataFrame.from_csv('xmrbtc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, output_size=3):\n",
    "        super(TNetwork, self).__init__()\n",
    "        self.full1 =  nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.full2 =  nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.Linear(output_size, 3)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        print(input)\n",
    "        print(hidden)\n",
    "        combined = torch.cat((input, hidden),1)\n",
    "        hidden = self.full1(combined)\n",
    "        output = self.full2(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)).type(FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def next(self):\n",
    "        if self.index >= len(self.data):\n",
    "            self.index = 0\n",
    "        d = self.data[self.index]\n",
    "        self.index += 1\n",
    "        return d\n",
    "    \n",
    "    def current(self):\n",
    "        return self.data[self.index]\n",
    "    \n",
    "    def index(self):\n",
    "        return self.index\n",
    "    \n",
    "    def has_next(self):\n",
    "        return self.index < len(self.data)\n",
    "\n",
    "it = DataIter(data)\n",
    "def get_next_data():\n",
    "    return torch.from_numpy(it.next()).unsqueeze(0).type(Tensor)\n",
    "\n",
    "class MoneySimulator:\n",
    "    def __init__(self):\n",
    "        self.monero = 0.1\n",
    "        self.btcMax = self.monero * prices['close'].iloc[0]\n",
    "        self.btc = 0\n",
    "        \n",
    "    def step(self,action):\n",
    "        if action == 1:\n",
    "            return (0, False)\n",
    "        \n",
    "        if action == 0 and self.monero == 0:\n",
    "            return  (0, False)\n",
    "        \n",
    "        if action == 2 and self.btc == 0:\n",
    "            return  (0, False)\n",
    "        \n",
    "        if action == 0:\n",
    "            self.monero = self.monero / 2\n",
    "            xmr_p = self.monero\n",
    "            self.btc = xmr_p * prices['close'].iloc[it.index + 1]\n",
    "            \n",
    "            current = self.monero / prices['close'].iloc[it.index + 1] + self.btc\n",
    "            \n",
    "            self.btcMax = self.btcMax if current < self.btcMax else current\n",
    "            return self.calculate_res()\n",
    "        \n",
    "        if action == 2:\n",
    "            self.btc = self.btc / 2\n",
    "            btc = self.btc\n",
    "            self.monero = btc / prices['close'].iloc[it.index + 1]\n",
    "            \n",
    "            current = self.monero / prices['close'].iloc[it.index + 1] + self.btc\n",
    "            \n",
    "            self.btcMax = self.btcMax if current < self.btcMax else current\n",
    "            return self.calculate_res()\n",
    "        \n",
    "    def calculate_res(self):\n",
    "        def calculate_at_moment(price):\n",
    "            btc = self.monero / price\n",
    "            return (btc + self.btc) - self.btcMax\n",
    "        \n",
    "        return (calculate_at_moment(prices['close'].iloc[it.index + 2]), False)\n",
    "        \n",
    "        \n",
    "sim = MoneySimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "model = TNetwork(len(data[0]))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "    \n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        output, hidden = model(\n",
    "            Variable(state, volatile=True).type(FloatTensor),model.initHidden())\n",
    "        return data.data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(3)]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)))\n",
    "\n",
    "    # We don't want to backprop through the expected action values and volatile\n",
    "    # will save us on temporarily changing the model parameters'\n",
    "    # requires_grad to False!\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "    # Now, we don't want to mess up the loss with a volatile flag, so let's\n",
    "    # clear it. After this, we'll just end up with a Variable that has\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      "-3.0000e-01 -4.1000e-01  1.0000e-05 -1.8820e+01 -1.4900e+00  3.2000e-01\n",
      "\n",
      "Columns 6 to 11 \n",
      " 2.0000e-05 -6.5000e-01 -2.1100e+00  3.7000e-01  1.0000e-05  2.7500e+00\n",
      "\n",
      "Columns 12 to 17 \n",
      "-2.2300e+00 -9.2000e-01  2.0000e-05 -3.7420e+01 -5.3036e+02 -1.1074e+02\n",
      "\n",
      "Columns 18 to 23 \n",
      "-2.6269e+02 -6.0819e+05 -4.1098e+05 -4.3952e+04 -6.8400e+00 -1.9210e+01\n",
      "\n",
      "Columns 24 to 29 \n",
      " 1.1000e-01 -4.7000e-01  2.0000e-05 -1.8260e+01 -6.0000e-01 -2.8000e-01\n",
      "\n",
      "Columns 30 to 33 \n",
      "-1.0000e-05  1.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.cuda.FloatTensor of size 1x34 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 129 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 130 to 142 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 143 to 155 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 156 to 168 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 169 to 181 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 182 to 194 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 195 to 207 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 208 to 220 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 221 to 233 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 234 to 246 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 247 to 255 \n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 1x256 (GPU 0)]\n",
      "\n",
      "\n",
      " 13\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c09eddcabcb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    last_point = get_next_data()\n",
    "    current_point = get_next_data()\n",
    "    state = current_point - last_point\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        print(action)\n",
    "        reward, done = sim.step(action[0, 0])\n",
    "        reward = Tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        last_point = current_point\n",
    "        current_point = get_next_data()\n",
    "        if not done:\n",
    "            next_state = current_point - last_point\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
